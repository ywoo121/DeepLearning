{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9733231,"sourceType":"datasetVersion","datasetId":5956638}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-27T13:16:39.936089Z\",\"iopub.execute_input\":\"2024-10-27T13:16:39.936719Z\",\"iopub.status.idle\":\"2024-10-27T13:16:39.944301Z\",\"shell.execute_reply.started\":\"2024-10-27T13:16:39.936672Z\",\"shell.execute_reply\":\"2024-10-27T13:16:39.942911Z\"}}\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\nfrom PIL import Image\nimport re    \nimport matplotlib.pyplot as plt\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-27T13:16:39.946586Z\",\"iopub.execute_input\":\"2024-10-27T13:16:39.947065Z\",\"iopub.status.idle\":\"2024-10-27T13:16:39.972329Z\",\"shell.execute_reply.started\":\"2024-10-27T13:16:39.947006Z\",\"shell.execute_reply\":\"2024-10-27T13:16:39.970970Z\"}}\n\nclass CustomDataset:\n    def __init__(self, base_dir):\n        \"\"\"\n        CustomDataset 클래스 초기화 함수\n        - 기능: 데이터 경로를 설정하고 데이터 로딩을 위한 변수들을 초기화\n        - 매개변수: base_dir (str) - 데이터셋의 기본 디렉토리 경로\n        \"\"\"\n        self.image_dir = os.path.join(base_dir, \"Images/Images\")\n        self.json_dir = os.path.join(base_dir, \"Images/json_files\")\n        self.label_file = os.path.join(base_dir, \"G1020.csv\")\n        self.labels_df = None\n        self.X = None\n        self.y = None\n\n    def load_data(self):\n        \"\"\"\n        레이블 데이터를 불러오는 함수\n        - 기능: CSV 파일을 읽어와서 이미지 ID와 라벨을 추출하여 저장\n        - 매개변수: 없음\n        - 리턴값: 없음\n        \"\"\"\n        self.labels_df = pd.read_csv(self.label_file)\n        self.labels_df['imageID'] = self.labels_df['imageID'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n        self.X = self.labels_df['imageID'].values\n        self.y = self.labels_df['binaryLabels'].values\n\n    def custom_train_test_split(self, test_size=0.2, random_state=None, shuffle=True):\n        \"\"\"\n        데이터셋을 훈련 및 테스트 세트로 분리하는 함수\n        - 기능: 데이터를 무작위로 섞고 주어진 비율로 훈련 및 테스트 세트로 분할\n        - 매개변수: \n            - test_size (float) - 테스트 세트 비율 (기본값 0.2)\n            - random_state (int) - 무작위 시드 값 (기본값 없음)\n            - shuffle (bool) - 셔플 여부 (기본값 True)\n        - 리턴값: X_train, X_test, y_train, y_test - 분할된 데이터셋\n        \"\"\"\n        if random_state is not None:\n            np.random.seed(random_state)\n\n        n_samples = len(self.X)\n        indices = np.arange(n_samples)\n\n        if shuffle:\n            np.random.shuffle(indices)\n\n        test_size = int(n_samples * test_size)\n        train_size = n_samples - test_size\n\n        X_train = self.X[indices[:train_size]]\n        X_test = self.X[indices[train_size:]]\n        y_train = self.y[indices[:train_size]]\n        y_test = self.y[indices[train_size:]]\n\n        return X_train, X_test, y_train, y_test\n\n    def calculate_vertical_diameter(self, points):\n        \"\"\"\n        주어진 좌표의 수직 직경을 계산하는 함수\n        - 기능: `y` 좌표의 최댓값과 최솟값의 차이를 계산하여 수직 직경을 구함\n        - 매개변수: points (list) - (x, y) 좌표의 리스트\n        - 리턴값: 수직 직경 (float)\n        \"\"\"\n        y_coords = [y for x, y in points]\n        return max(y_coords) - min(y_coords)\n\n    def calculate_cdr(self, json_path):\n        \"\"\"\n        CDR (Cup-to-Disc Ratio)을 계산하는 함수\n        - 기능: disc와 cup 직경을 불러와 CDR 값을 계산\n        - 매개변수: json_path (str) - JSON 파일 경로\n        - 리턴값: CDR 값 (float)\n        \"\"\"\n        with open(json_path, 'r') as file:\n            annotation = json.load(file)\n\n        od_diameter, oc_diameter = 0, 0\n        for shape in annotation['shapes']:\n            points = shape['points']\n            if shape['label'] == 'disc':\n                od_diameter = self.calculate_vertical_diameter(points)\n            elif shape['label'] == 'cup':\n                oc_diameter = self.calculate_vertical_diameter(points)\n\n        return oc_diameter / od_diameter if od_diameter > 0 else 0\n\n    def preprocess_vcdr(self, vcdr_value, min_val=0, max_val=1):\n        \"\"\"\n        CDR 값 정규화\n        - 기능: 주어진 CDR 값을 0과 1 사이의 범위로 정규화\n        - 매개변수:\n            - vcdr_value: CDR 값\n            - min_val: 최소값 (기본값: 0)\n            - max_val: 최대값 (기본값: 1)\n        - 리턴값: 정규화된 CDR 값\n        \"\"\"\n        return (vcdr_value - min_val) / (max_val - min_val) if min_val <= vcdr_value <= max_val else vcdr_value\n\n    def load_image_and_vcdr(self, image_id, img_size=(64, 64)):\n        \"\"\"\n        이미지와 CDR 값 불러오기\n        - 기능: 주어진 이미지 ID에 해당하는 이미지와 CDR 값을 불러오고 전처리\n        - 매개변수:\n            - image_id: 이미지 ID\n            - img_size: 이미지 크기 (기본값: 64x64)\n        - 리턴값: 전처리된 이미지 배열, 정규화된 CDR 값\n        \"\"\"\n        img_path = os.path.join(self.image_dir, f\"image_{image_id}.jpg\")\n        json_path = os.path.join(self.json_dir, f\"image_{image_id}.json\")\n\n        img = Image.open(img_path).convert('L').resize(img_size)\n        img = np.array(img) / 255.0\n\n        vcdr = self.calculate_cdr(json_path)\n        normalized_vcdr = self.preprocess_vcdr(vcdr)\n\n        return img, normalized_vcdr\n\n    def prepare_data(self, X_ids):\n                \"\"\"\n        이미지 데이터 전처리 준비\n        - 기능: 주어진 이미지 ID 리스트에 해당하는 이미지 배열과 CDR 값 반환\n        - 매개변수:\n            - X_ids: 이미지 ID 리스트\n        - 리턴값: 전처리된 이미지 배열과 CDR 값 배열\n        \"\"\"\n        X, vCDR = [], []\n        for image_id in X_ids:\n            img, vcdr = self.load_image_and_vcdr(image_id)\n            X.append(img)\n            vCDR.append(vcdr)\n        return np.expand_dims(np.array(X), axis=-1), np.array(vCDR)\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-27T13:16:40.145928Z\",\"iopub.execute_input\":\"2024-10-27T13:16:40.146367Z\",\"iopub.status.idle\":\"2024-10-27T13:16:40.178245Z\",\"shell.execute_reply.started\":\"2024-10-27T13:16:40.146326Z\",\"shell.execute_reply\":\"2024-10-27T13:16:40.176934Z\"}}\nclass SimpleCNN:\n    def __init__(self, input_shape, learning_rate=0.001, reg_lambda=0.001):\n        \"\"\"\n        SimpleCNN 클래스 초기화\n        - 기능: 모델의 입력 형태, 학습률, L2 정규화 계수, 가중치 초기화\n        - 매개변수:\n            - input_shape: 입력 데이터의 형태\n            - learning_rate: 학습률 (기본값: 0.001)\n            - reg_lambda: L2 정규화 계수 (기본값: 0.001)\n        \"\"\"\n        self.input_shape = input_shape\n        self.learning_rate = learning_rate\n        self.reg_lambda = reg_lambda\n        self.weights_conv1 = np.random.randn(3, 3, 1) * np.sqrt(1.0 / (3 * 3))  # Xavier 초기화\n        self.flat_shape = 961\n        self.weights_dense = np.random.randn(self.flat_shape + 1) * np.sqrt(1.0 / (self.flat_shape + 1))  # Xavier 초기화\n        self.bias_dense = np.random.randn(1) * 0.01\n\n    def conv2d(self, input_image, kernel, stride=1, padding=0):\n        \"\"\"\n        2D 합성곱 연산\n        - 기능: 입력 이미지에 대해 2D 합성곱 연산 수행\n        - 매개변수:\n            - input_image: 입력 이미지 배열\n            - kernel: 필터 커널 배열\n            - stride: 스트라이드 (기본값: 1)\n            - padding: 패딩 (기본값: 0)\n        - 리턴값: 합성곱 연산 결과 배열\n        \"\"\"\n        kernel_size = kernel.shape[0]\n        output_size = (input_image.shape[0] - kernel_size + 2 * padding) // stride + 1\n        output = np.zeros((output_size, output_size))\n        padded_image = np.pad(input_image, [(padding, padding)], mode='constant')\n\n        for i in range(output_size):\n            for j in range(output_size):\n                region = padded_image[i * stride:i * stride + kernel_size, j * stride:j * stride + kernel_size]\n                output[i, j] = np.sum(region * kernel)\n\n        return output\n\n    def relu(self, x):\n        \"\"\"\n        ReLU 활성화 함수\n        - 기능: ReLU 함수로 음수 값 0으로 변환\n        - 매개변수:\n            - x: 입력 배열\n        - 리턴값: ReLU 적용된 배열\n        \"\"\"\n        return np.maximum(0, x)\n\n    def max_pool(self, input_image, pool_size=2, stride=2):\n        \"\"\"\n        최대 풀링 연산\n        - 기능: 입력 이미지에 대해 최대 풀링 연산을 수행하여 크기를 줄임\n        - 매개변수:\n            - input_image: 입력 이미지 배열\n            - pool_size: 풀링 크기 (기본값: 2)\n            - stride: 풀링 스트라이드 (기본값: 2)\n        - 리턴값: 풀링이 적용된 배열\n        \"\"\"\n        output_size = input_image.shape[0] // stride\n        output = np.zeros((output_size, output_size))\n\n        for i in range(0, input_image.shape[0], stride):\n            for j in range(0, input_image.shape[1], stride):\n                output[i // stride, j // stride] = np.max(input_image[i:i + pool_size, j:j + pool_size])\n\n        return output\n\n    def dense_layer(self, input_vector, weights, bias):\n        \"\"\"\n        밀집층 (Dense Layer) 연산\n        - 기능: 입력 벡터에 대해 가중치와 바이어스를 사용한 밀집층 연산 수행, 시그모이드 활성화 함수 사용\n        - 매개변수:\n            - input_vector: 평탄화된 입력 벡터\n            - weights: 밀집층 가중치\n            - bias: 밀집층 바이어스\n        - 리턴값: 활성화 함수가 적용된 출력 값\n        \"\"\"\n        return 1 / (1 + np.exp(-(input_vector.dot(weights) + bias)))\n\n    def mean_squared_error(self, y_true, y_pred):\n        \"\"\"\n        평균 제곱 오차 (MSE) 손실 함수\n        - 기능: 예측값과 실제값 간의 평균 제곱 오차를 계산하고 L2 정규화를 추가하여 손실 계산\n        - 매개변수:\n            - y_true: 실제 라벨 값\n            - y_pred: 예측 값\n        - 리턴값: 평균 제곱 오차 손실 값\n        \"\"\"\n        return np.square(y_true - y_pred).mean() + self.reg_lambda * np.sum(np.square(self.weights_dense))  # L2 정규화 추가\n\n    def forward(self, X, vCDR):\n        \"\"\"\n        전방향 전달 (Forward Pass)\n        - 기능: 입력 이미지와 CDR 값을 통해 합성곱, ReLU, 풀링, 밀집층 순으로 전방향 전달 수행\n        - 매개변수:\n            - X: 입력 이미지\n            - vCDR: CDR 값\n        - 리턴값: 최종 출력 값 (예측 확률)\n        \"\"\"\n        conv1 = self.conv2d(X.squeeze(), self.weights_conv1)\n        relu1 = self.relu(conv1)\n        pool1 = self.max_pool(relu1)\n        flat = pool1.flatten()\n        input_vector = np.append(flat, vCDR)\n        return self.dense_layer(input_vector, self.weights_dense, self.bias_dense)\n\n    def train(self, X_train, vCDR_train, y_train, X_val, vCDR_val, y_val, epochs, decay=0.9, batch_size=32):\n        \"\"\"\n        모델 훈련\n        - 기능: 주어진 훈련 데이터로 모델을 학습하고, 에포크마다 손실 값과 검증 정확도 계산\n        - 매개변수:\n            - X_train: 훈련 이미지 데이터\n            - vCDR_train: 훈련 CDR 데이터\n            - y_train: 훈련 라벨\n            - X_val: 검증 이미지 데이터\n            - vCDR_val: 검증 CDR 데이터\n            - y_val: 검증 라벨\n            - epochs: 훈련 에포크 수\n            - decay: 학습률 감소 계수 (기본값: 0.9)\n            - batch_size: 배치 크기 (기본값: 32)\n        - 리턴값: 훈련 손실 목록, 검증 정확도 목록\n        \"\"\"\n        train_losses, val_accuracies = [], []\n        n_samples = len(X_train)\n\n        for epoch in range(epochs):\n            total_loss = 0\n            batch_indices = np.random.permutation(n_samples)  # 샘플 순서를 무작위로 섞기\n            for i in range(0, n_samples, batch_size):\n                batch_idx = batch_indices[i:i+batch_size]\n                X_batch, vCDR_batch, y_batch = X_train[batch_idx], vCDR_train[batch_idx], y_train[batch_idx]\n                \n                batch_loss = 0\n                for j in range(len(X_batch)):\n                    y_pred = self.forward(X_batch[j], vCDR_batch[j])\n                    loss = self.mean_squared_error(y_batch[j], y_pred)\n                    batch_loss += loss\n\n                    # Backpropagation 및 가중치 업데이트\n                    input_vector = np.append(self.max_pool(self.relu(self.conv2d(X_batch[j].squeeze(), self.weights_conv1))).flatten(), vCDR_batch[j])\n                    self.weights_dense -= self.learning_rate * (y_pred - y_batch[j]) * input_vector\n                    self.bias_dense -= self.learning_rate * (y_pred - y_batch[j])\n\n                total_loss += batch_loss / batch_size\n\n            # 학습률 감소\n            self.learning_rate *= decay\n\n            # 검증 정확도 계산\n            correct_predictions = sum(1 for i in range(len(X_val)) if (self.forward(X_val[i], vCDR_val[i]) >= 0.5) == y_val[i])\n            accuracy = correct_predictions / len(X_val)\n\n            train_losses.append(total_loss / (n_samples / batch_size))\n            val_accuracies.append(accuracy)\n\n            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {train_losses[-1]}, Validation Accuracy: {accuracy}\")\n\n        return train_losses, val_accuracies\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-27T13:16:40.181052Z\",\"iopub.execute_input\":\"2024-10-27T13:16:40.181639Z\",\"iopub.status.idle\":\"2024-10-27T13:16:40.195510Z\",\"shell.execute_reply.started\":\"2024-10-27T13:16:40.181564Z\",\"shell.execute_reply\":\"2024-10-27T13:16:40.193996Z\"}}\ndef plot_results(train_losses, val_accuracies, epochs):\n    \"\"\"\n    훈련 결과 시각화\n    - 기능: 훈련 손실과 검증 정확도를 플롯하여 시각적으로 표시\n    - 매개변수:\n        - train_losses: 훈련 손실 목록\n        - val_accuracies: 검증 정확도 목록\n        - epochs: 에포크 수\n    - 리턴값: 없음\n    \"\"\"\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(1, epochs+1), train_losses)\n    plt.title(\"Training Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n\n    plt.subplot(1, 2, 2)\n    plt.plot(range(1, epochs+1), val_accuracies)\n    plt.title(\"Validation Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n\n    plt.tight_layout()\n    plt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-10-27T13:16:40.197271Z\",\"iopub.execute_input\":\"2024-10-27T13:16:40.197676Z\",\"iopub.status.idle\":\"2024-10-27T13:33:57.862682Z\",\"shell.execute_reply.started\":\"2024-10-27T13:16:40.197630Z\",\"shell.execute_reply\":\"2024-10-27T13:33:57.861163Z\"}}\n# 메인 실행 부분\nif __name__ == \"__main__\":\n    base_dir = \"/kaggle/input/g1020-data/G1020\"\n    dataset = CustomDataset(base_dir)\n    dataset.load_data()\n\n    X_train_ids, X_val_ids, y_train, y_val = dataset.custom_train_test_split(test_size=0.2, random_state=42)\n\n    X_train, vCDR_train = dataset.prepare_data(X_train_ids)\n    X_val, vCDR_val = dataset.prepare_data(X_val_ids)\n\n    model = SimpleCNN(input_shape=(64, 64, 1), learning_rate=0.0005)\n    epochs = 10\n    train_losses, val_accuracies = model.train(X_train, vCDR_train, y_train, X_val, vCDR_val, y_val, epochs)\n\n    plot_results(train_losses, val_accuracies, epochs)","metadata":{"_uuid":"92225607-3024-48ea-a0b6-a8c6bae5f843","_cell_guid":"f680ada9-57ca-450b-84f2-8bc49ad0314e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}